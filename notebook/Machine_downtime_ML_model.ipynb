{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, StratifiedShuffleSplit,\\\n",
    "                                    cross_val_predict\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score,\\\n",
    "                            recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "machine = pd.read_csv(\"../data/machine_downtime_cleaned.csv\", parse_dates=['Date'])\n",
    "\n",
    "# make a copy of the data \n",
    "machine_ori = machine.copy()\n",
    "# print the first few rows\n",
    "machine.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "we have to divide the numeric columns into those that are skewed and those that are normal in order to be able to apply the necessary standardization or normalization to avoid bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store columns that are normally or\n",
    "# skewly distributed\n",
    "normal_cols = []\n",
    "skewed_cols = []\n",
    "\n",
    "# loop through the numerical features\n",
    "for col in machine_ori.select_dtypes(include=np.number):\n",
    "    skewness = machine_ori[col].skew()\n",
    "    kurtosis = machine_ori[col].kurt()\n",
    "\n",
    "    # set a threshold for kurtosis and skewness and then append the necessary features\n",
    "    if -0.2 <= skewness <= 0.3 and -0.2 <= kurtosis <= 0.2:  # Adjust thresholds as needed\n",
    "        normal_cols.append(col)\n",
    "        print(f\"{col}: Skewness = {skewness:.2f}, Kurtosis = {kurtosis:.2f} (Approximately Normal)\")\n",
    "    else:\n",
    "        skewed_cols.append(col)\n",
    "        print(f\"{col}: Skewness = {skewness:.2f}, Kurtosis = {kurtosis:.2f} (Not Normally Distributed)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "X = machine_ori.drop(columns=[\"Downtime\", \"Machine_ID\", \"Date\", \"Assembly_Line_No\"])  # Features\n",
    "\n",
    "# define encoder\n",
    "label_encode = LabelEncoder()\n",
    "y = label_encode.fit_transform(machine_ori[\"Downtime\"])  # Target variable\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Define transformers\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"robust\", RobustScaler(), skewed_cols),  # Skewed data\n",
    "    (\"standard\", StandardScaler(), normal_cols)  # Normal data \n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    stratify = y, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Bayesian Logistic Regression\": LogisticRegression(solver=\"lbfgs\"),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"auc\", random_state = 42)\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model \n",
    "#### Cross Validation\n",
    "\n",
    "Since our problem is a classification task, Stratified K-Fold (StratifiedKFold) will be use for the cross validation. \n",
    "\n",
    "Why Use Stratified K-Fold?\n",
    "\n",
    "+ Preserves Class Distribution: Stratified K-Fold ensures that each fold maintains the same proportion of classes as the overall dataset, which is crucial when dealing with classification problems, even if there is no visible class imbalance.\n",
    "+ More Reliable Performance Estimates: It provides a more stable and representative estimate of your model’s performance compared to ShuffleSplit, which may produce folds with different class distributions.\n",
    "+ Better Generalization: Ensures that all classes are well represented in training and validation splits, reducing the risk of biased results.\n",
    "\n",
    "**Key Performance Metrics and Their Meaning**\n",
    "\n",
    "+ Precision: Measures how many of the predicted failures were actually failures. A high precision means fewer false positives.\n",
    "+ Recall: Measures how many of the actual failures were correctly identified. A high recall means fewer false negatives.\n",
    "+ F1-Score: Harmonic mean of precision and recall, balancing both. Higher is better.\n",
    "+ ROC AUC: Measures the model’s ability to distinguish between classes. A value closer to 1 is better.\n",
    "\n",
    "**Model Comparison and Best Performing Model**\n",
    "\n",
    "Model Performance Interpretation:\n",
    "\n",
    "1. Best Overall Model: XGBoost (0.9993 ROC AUC, 0.9869 F1-Score)\n",
    "\n",
    "+ Highest ROC AUC (0.9993) → Best discrimination ability.\n",
    "+ Very high precision (0.9934) → Almost all predicted failures were actual failures.\n",
    "+ Very high recall (0.9805) → Nearly all actual failures were correctly identified.\n",
    "+ Strong balance between precision & recall (F1-Score = 0.9869).\n",
    "\n",
    "Likely the best choice for deployment.\n",
    "\n",
    "2. Random Forest is also very strong (0.9989 ROC AUC, 0.9870 F1-Score)\n",
    "\n",
    "> + Very similar performance to XGBoost.\n",
    "> + If interpretability is needed, Random Forest may be preferable.\n",
    "\n",
    "3. Gradient Boosting also performs well (0.9981 ROC AUC, 0.9853 F1-Score)\n",
    "\n",
    "> + Close competitor but slightly lower recall than XGBoost.\n",
    "\n",
    "4. Decision Tree (0.9647 ROC AUC, 0.9644 F1-Score)\n",
    "\n",
    "Still good but lacks the power of ensemble methods.\n",
    "\n",
    "5. SVM & Bayesian Logistic Regression are weaker\n",
    "\n",
    "> + SVM (0.9469 ROC AUC, 0.8696 F1-Score) and Bayesian Logistic Regression (0.9125 ROC AUC, 0.8419 F1-Score) underperform compared to ensemble models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\machineind\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:50:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# craete an empty list to store model result\n",
    "model_results = []\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# iterate through the models\n",
    "for name, model in models.items():\n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        \n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "        \n",
    "        ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    y_pred_cv = cross_val_predict(pipeline, X_train, y_train, cv=cv)\n",
    "    y_prob_cv = cross_val_predict(pipeline, X_train, y_train, cv=cv, method=\"predict_proba\")\\\n",
    "                [:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # evaluate Metrics\n",
    "    precision = precision_score(y_train, y_pred_cv)\n",
    "    recall = recall_score(y_train, y_pred_cv)\n",
    "    f1 = f1_score(y_train, y_pred_cv)\n",
    "    roc_auc = roc_auc_score(y_train, y_prob_cv) if y_prob_cv is not None else 'N/A'\n",
    "    \n",
    "    # append result\n",
    "    model_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-Score\": round(f1, 4),\n",
    "        \"ROC AUC\": round(roc_auc, 4) if roc_auc != \"N/A\" else \"N/A\"\n",
    "    })\n",
    "    \n",
    "    # convert result to Datframe\n",
    "    model_results_df = pd.DataFrame(model_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayesian Logistic Regression</td>\n",
       "      <td>0.8565</td>\n",
       "      <td>0.8547</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>0.9287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.9684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.8839</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Precision  Recall  F1-Score  ROC AUC\n",
       "0  Bayesian Logistic Regression     0.8565  0.8547    0.8556   0.9287\n",
       "1                 Random Forest     0.9828  0.9902    0.9865   0.9988\n",
       "2             Gradient Boosting     0.9892  0.9957    0.9924   0.9992\n",
       "3                 Decision Tree     0.9665  0.9696    0.9681   0.9684\n",
       "4                           SVM     0.8839  0.8753    0.8796   0.9457\n",
       "5                       XGBoost     0.9903  0.9946    0.9924   0.9992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Class Imbalance\n",
    "\n",
    "I have trained all the models involved, and most of them exhibit exceptionally high evaluation metric values, reaching as high as 0.99. Given that this is a classification problem, one potential concern could be class imbalance, which often leads to inflated performance metrics. However, after thoroughly checking the class distribution, there doesn’t appear to be any significant imbalance. This suggests that the models might either be capturing strong patterns in the data or potentially overfitting. Further investigation, such as cross-validation performance consistency and feature importance analysis will be implemented to ensure the models’ generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_ori['Downtime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stratified K-Fold Cross Validation\n",
    "n_splits = 5\n",
    "strat_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store AUC scores for each fold\n",
    "roc_scores = []\n",
    "\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for train_idx, val_idx in strat_kfold.split(X_train, y_train):\n",
    "    # Split data\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Define the model\n",
    "    model_cross = xgb.XGBClassifier(\n",
    "        n_estimators=300,  # Fixed number of trees\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=1,\n",
    "       # use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model_cross.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], \n",
    "                verbose=False)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred = model_cross.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Compute ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    roc_scores.append(roc_auc)\n",
    "\n",
    "# Print the average ROC-AUC score\n",
    "mean_auc = np.mean(roc_scores)\n",
    "print(f\"Mean ROC AUC across {n_splits} folds: {mean_auc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train and y_train to Pandas DataFrames/Series if needed\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    # instantiate the kfold\n",
    "    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    roc_auc_scores = []    # instatntiate an empty list to store the roc_auc scores\n",
    "\n",
    "    for train_index, val_index in strat_kfold.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[train_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        # Ensure X_train and y_train are Pandas DataFrame/Series\n",
    "\n",
    "        #print(X_train_fold.shape, X_val_fold.shape, y_train_fold.shape, y_val_fold.shape)\n",
    "        y_train_fold = y_train_fold.values.ravel()\n",
    "        y_val_fold = y_val_fold.values.ravel()\n",
    "\n",
    "    \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                    eval_set = [(X_val_fold, y_val_fold)],\n",
    "                     verbose = False)\n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        roc_auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n",
    "    return mean(roc_auc_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials = 50, timeout=1800) # run 50 trials or max 30mins\n",
    "\n",
    "# print best params\n",
    "print('Best Parameters found: ', study.best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machineind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
